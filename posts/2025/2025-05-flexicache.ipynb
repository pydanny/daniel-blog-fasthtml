{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885d07f9",
   "metadata": {},
   "source": [
    "---\n",
    "date: \"2025-05-09T08:00:00.00Z\"\n",
    "title: \"Exploring flexicache\"\n",
    "description: \"An exploration of using flexicache for caching in Python.\"\n",
    "published: true\n",
    "tags:\n",
    "  - python\n",
    "  - fastcore\n",
    "  - answerdotai\n",
    "time_to_read: 2\n",
    "type: post\n",
    "image: /public/images/exploring-flexicache.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6037755",
   "metadata": {},
   "source": [
    "When coding in Python I find I really like to use decorators to cache results from functions and methods, often to memory and sometimes to ephemeral stores like memcached. In fact, I've worked on and created several cache decorators, including [one](https://pypi.org/project/cached-property/) that influenced the implementation of the `@cached_property` decorator in Python 3.8. \n",
    "\n",
    "A cache decorator called [flexicache](https://fastcore.fast.ai/xtras.html#flexicache) is part of the [fastcore](https://pypi.org/project/fastcore/) library. `flexicache` allows you to cache in memory results from functions and methods in a flexible way. Besides having an implementation of LRU caching, each use of the decorator can be configured to use one or more cache invalidation policies. \n",
    "\n",
    "Two policies, `time_policy` and `mtime_policy` are used to invalidate the cache based on time and file modification time respectively. The `time_policy` invalidates the cache after a specified number of seconds, while the `mtime_policy` invalidates the cache if the file has been modified since the last time it was cached.\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe2ee8",
   "metadata": {},
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c73ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from fastcore.xtras import flexicache, time_policy, mtime_policy\n",
    "# Libraries used in testing cache validity and cache invalidation\n",
    "from random import randint\n",
    "from pathlib import Path\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70758f7",
   "metadata": {},
   "source": [
    "Here's a simple function returning a number between 1 to 1000 that we can show being cached. We'll use this in all our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f606e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_func(v):\n",
    "    return randint(1, 1000)\n",
    "\n",
    "# Assert False as the function is not cached\n",
    "assert random_func(1) != random_func(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c74eab",
   "metadata": {},
   "source": [
    "### Time policy\n",
    "\n",
    "This is how we use the `time_policy` to cache the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01de4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flexicache(time_policy(.1))\n",
    "def random_func():\n",
    "    return randint(1, 1000) \n",
    "\n",
    "# assert True as the function is cached\n",
    "assert random_func() == random_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c047181",
   "metadata": {},
   "source": [
    "Let's use the sleep function to simulate time between calls to `random_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a90b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = random_func()\n",
    "# True as the function is cached \n",
    "assert result == random_func()  \n",
    "# Sleep for .2 seconds to allow cache to expire\n",
    "sleep(0.2)  \n",
    "# Assert False as the cache has expired and the function is called again\n",
    "assert result != random_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a01db",
   "metadata": {},
   "source": [
    "### File modification time (mtime_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e3f72",
   "metadata": {},
   "source": [
    "We'll try with `mtime_policy`, checking to see if touching a file invalidates the cache. We'll use this site's `main.py` file as the file to touch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d971cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flexicache(mtime_policy('../../main.py'))\n",
    "def random_func():\n",
    "    return randint(1, 1000)\n",
    "\n",
    "# Assert True as the function is cached\n",
    "assert random_func() == random_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea618d8e",
   "metadata": {},
   "source": [
    "Now let's use the Path.touch() method to touch the file. This will update the file's modification time to the current time, which should invalidate the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fd86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to cache the result\n",
    "result = random_func() \n",
    "assert result == random_func()  # True as the function is cached \n",
    "# Update the file's modification time, which invalidates the cache\n",
    "Path('../../main.py').touch()  \n",
    "# Assert False as the cache is invalidated\n",
    "assert result != random_func()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc263ccf",
   "metadata": {},
   "source": [
    "## Using multiple policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0c3ce",
   "metadata": {},
   "source": [
    "A unique feature of `flexicache` is that you can use multiple policies at the same time. This allows you to combine the benefits of different caching strategies.\n",
    "In this example, we'll use both `time_policy` and `mtime_policy` together. This means that the cache will be invalidated if either the time limit is reached or the file has been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812716d",
   "metadata": {},
   "source": [
    "Testing the cache with both policies is identical to the previous examples. We'll call the function, first with the time policy, then with the mtime policy, and finally with both policies. We'll also touch the file to see if it invalidates the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e319ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flexicache(time_policy(.1), mtime_policy('../../main.py'))\n",
    "def random_func():\n",
    "    return randint(1, 1000)\n",
    "\n",
    "# True as the function is cached\n",
    "assert random_func() == random_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318606df",
   "metadata": {},
   "source": [
    "Testing time invalidation is the same as before. We'll call the function, wait for the time limit to be reached, and then call it again to see if the cache is invalidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c78322",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = random_func()\n",
    "# True as the function is cached \n",
    "assert result == random_func()  \n",
    "# Sleep for .2 seconds to allow cache to expire\n",
    "sleep(0.2)  \n",
    "# False as the cache has expired and the function is called again\n",
    "assert result != random_func() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a92dd",
   "metadata": {},
   "source": [
    "Testing file timestamp is the same as before. We'll call the function, touch the file, and then call it again to see if the cache is invalidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb53d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to cache the result\n",
    "result = random_func() \n",
    "# True as the function is cached \n",
    "assert result == random_func()  \n",
    "# Update the file's modification time, which invalidates the cache\n",
    "Path('../../main.py').touch()  \n",
    "# Assert False as the cache is invalidated\n",
    "assert result != random_func()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1cc0e8",
   "metadata": {},
   "source": [
    "## What about LRU caching?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae7626",
   "metadata": {},
   "source": [
    "Now let's test out the `flexicache` decorator to see how it behaves as an [lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache) replacement. For reference, LRU caching is a caching strategy that keeps track of the most recently used items and removes the least recently used items when the cache reaches its maximum size. In other words, it takes out the latest items from the cache first when it runs out of space. It uses the [FIFO](https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)) (first in, first out) strategy to remove the oldest items from the cache.\n",
    "\n",
    "We'll use `flexicache` with `maxsize` (of cache) of 2, meaning after 2 saves it starts discarding the oldest cache entries. Entries in cache functions are identified in the cache by arguments (v),so we add an argument to the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419554c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flexicache(maxsize=2)\n",
    "def random_func(v):\n",
    "    return randint(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099e22d",
   "metadata": {},
   "source": [
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0fb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = random_func(1) \n",
    "# True as the function is cached\n",
    "assert result1 == random_func(1) \n",
    "# True as the function is cached\n",
    "assert random_func(2) == random_func(2)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4557bad",
   "metadata": {},
   "source": [
    "So far so good. The cache is working as expected. Now let's start evicting the first items added to the cache. We'll add a third item to the cache and see if the first one is evicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa6a3520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True as the function for 3 is cached,\n",
    "# but it will evict the result of  random_func2(1) \n",
    "assert random_func(3) == random_func(3)  \n",
    "# False as the first result is no longer cached\n",
    "assert result1 != random_func(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d40bd",
   "metadata": {},
   "source": [
    "## timed_cache convenience wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50573750",
   "metadata": {},
   "source": [
    "`lru_cache` is a built-in Python decorator that provides a simple way to cache the results of a function. It uses a Least Recently Used (LRU) caching strategy, which means that it keeps track of the most recently used items as based on arguments and removes the least recently used items when the cache reaches its maximum size. In other words, it takes out the latest items from the cache first when it runs out of space.\n",
    "\n",
    "The downside is that it doesn't have a timeout feature, so if you want to cache results for a specific amount of time, you need to implement that yourself.\n",
    "\n",
    "`fastcore.xtras.timed_cache` is an implementation of `flexicache` that adds a timeout feature to `functools.lru_cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73228e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.xtras import timed_cache\n",
    "\n",
    "# shortcut for @flexicache(time_policy(.1), maxsize=2)\n",
    "@timed_cache(.1, maxsize=2)\n",
    "def random_func(v):\n",
    "    return randint(1, 1000)\n",
    "\n",
    "# True as the function is cached\n",
    "assert random_func(1) == random_func(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cd1ca",
   "metadata": {},
   "source": [
    "Testing the timeout is the same as before with `flexicache(time_policy(.1), maxsize=2)`. We'll call the function, wait for the timeout to be reached, and then call it again to see if the cache is invalidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81c22184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait long enough for the cache to expire\n",
    "sleep(0.2)\n",
    "# Assert False as the cache is time invalidated\n",
    "assert result1 != random_func(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7cac47",
   "metadata": {},
   "source": [
    "Finally, confirm that the LRU cache is removing the first cached item. This is the same LRU cache set of tests we used in the section above about LRU caching. Again, we'll add a third item to the cache and see if the first one is evicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db968ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = random_func(1) \n",
    "# True as the function is cached\n",
    "assert result1 == random_func(1) \n",
    "# True as the function is cached\n",
    "assert random_func(2) == random_func(2)  \n",
    "# True as the function for 3 is cached,\n",
    "# but it will evict the result of random_func2(1) \n",
    "assert random_func(3) == random_func(3)  \n",
    "# False as the first result is no longer cached\n",
    "assert result1 != random_func(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e7c3a",
   "metadata": {},
   "source": [
    "![/public/images/exploring-flexicache.png](/public/images/exploring-flexicache.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
